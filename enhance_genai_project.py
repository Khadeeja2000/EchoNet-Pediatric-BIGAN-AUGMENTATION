"""
ENHANCED GenAI Project: Actual Video Analysis
Masters-level implementation with real video content analysis
"""
import os
import argparse
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import cv2
from pathlib import Path


class VideoFeatureExtractor(nn.Module):
    """Extract features from video using CNN"""
    def __init__(self):
        super().__init__()
        # Simple CNN for feature extraction
        self.features = nn.Sequential(
            nn.Conv3d(1, 32, kernel_size=(3, 3, 3), padding=1),
            nn.ReLU(),
            nn.MaxPool3d(2),
            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1),
            nn.ReLU(),
            nn.MaxPool3d(2),
            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool3d((1, 1, 1))
        )
        self.fc = nn.Linear(128, 64)
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x


class ViewClassifier(nn.Module):
    """Classify view type from video features"""
    def __init__(self, num_views=2):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(32, num_views)
        )
    
    def forward(self, features):
        return self.classifier(features)


def extract_video_features(video_path, model, device, frames=16, size=64):
    """Extract features from actual video content"""
    cap = cv2.VideoCapture(video_path)
    frames_list = []
    
    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if total <= 0:
        total = frames
    
    indices = np.linspace(0, max(total - 1, 0), frames).astype(int)
    
    for i in indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, int(i))
        ret, frame = cap.read()
        if not ret:
            break
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        gray = cv2.resize(gray, (size, size))
        frames_list.append(gray)
    cap.release()
    
    if len(frames_list) == 0:
        frames_list = [np.zeros((size, size), dtype=np.uint8) for _ in range(frames)]
    if len(frames_list) < frames:
        frames_list += [frames_list[-1]] * (frames - len(frames_list))
    
    # Convert to tensor
    video = np.stack(frames_list[:frames], axis=0).astype(np.float32) / 255.0
    video = torch.from_numpy(video).unsqueeze(0).unsqueeze(0)  # (1, 1, T, H, W)
    video = video.to(device)
    
    # Extract features
    with torch.no_grad():
        features = model(video)
    
    return features.cpu().numpy()


def analyze_video_content(video_path):
    """Analyze video content using computer vision"""
    cap = cv2.VideoCapture(video_path)
    
    # Extract frames
    frames = []
    for i in range(10):  # Sample 10 frames
        ret, frame = cap.read()
        if not ret:
            break
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        frames.append(gray)
    cap.release()
    
    if len(frames) == 0:
        return {}
    
    # Analyze video properties
    analysis = {}
    
    # 1. Motion analysis (optical flow)
    if len(frames) >= 2:
        flow = cv2.calcOpticalFlowFarneback(frames[0], frames[1], None, 0.5, 3, 15, 3, 5, 1.2, 0)
        magnitude = np.sqrt(flow[..., 0]**2 + flow[..., 1]**2)
        analysis['motion_intensity'] = float(np.mean(magnitude))
        analysis['has_motion'] = analysis['motion_intensity'] > 1.0
    
    # 2. Brightness/contrast analysis
    mean_brightness = np.mean([np.mean(f) for f in frames])
    analysis['brightness'] = float(mean_brightness)
    analysis['is_bright'] = mean_brightness > 100
    
    # 3. Edge detection (structure analysis)
    edges = [cv2.Canny(f, 50, 150) for f in frames]
    edge_density = np.mean([np.sum(e > 0) / e.size for e in edges])
    analysis['edge_density'] = float(edge_density)
    analysis['has_structure'] = edge_density > 0.1
    
    # 4. Texture analysis
    textures = []
    for f in frames[:5]:
        # Simple texture measure using variance
        texture = np.var(f)
        textures.append(texture)
    analysis['texture_variance'] = float(np.mean(textures))
    
    # 5. Frame consistency (temporal analysis)
    if len(frames) >= 3:
        diffs = [np.mean(np.abs(frames[i] - frames[i+1])) for i in range(len(frames)-1)]
        analysis['frame_consistency'] = float(np.mean(diffs))
        analysis['is_consistent'] = analysis['frame_consistency'] < 20
    
    return analysis


def predict_view_from_content(video_path, view_classifier, feature_extractor, device):
    """Predict view type from video content (not metadata!)"""
    # Extract features from video
    features = extract_video_features(video_path, feature_extractor, device)
    features_tensor = torch.from_numpy(features).to(device)
    
    # Classify view
    with torch.no_grad():
        logits = view_classifier(features_tensor)
        probs = torch.softmax(logits, dim=1)
        predicted_view_idx = torch.argmax(probs, dim=1).item()
    
    view_names = ['A4C', 'PSAX']
    predicted_view = view_names[predicted_view_idx]
    confidence = probs[0][predicted_view_idx].item()
    
    return predicted_view, confidence


def enhanced_video_to_text(video_path, metadata_row=None):
    """Enhanced video-to-text with actual video analysis"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # 1. Analyze video content (actual pixel analysis)
    content_analysis = analyze_video_content(video_path)
    
    # 2. Extract features (if models available)
    # In real implementation, would use trained models
    
    # 3. Generate description based on analysis
    description_parts = ["Echocardiogram video"]
    
    # Add content-based observations
    if content_analysis.get('has_motion'):
        description_parts.append("showing cardiac motion")
    if content_analysis.get('has_structure'):
        description_parts.append("with visible structures")
    if content_analysis.get('is_consistent'):
        description_parts.append("with consistent frames")
    
    # Add metadata if available (but acknowledge it's metadata)
    if metadata_row is not None:
        view = metadata_row.get('view', 'unknown')
        sex = metadata_row.get('sex', 'unknown')
        age_bin = metadata_row.get('age_bin', 'unknown')
        sex_str = "Female" if str(sex).lower().startswith('f') else "Male"
        description_parts.append(f"{view} view, {sex_str} patient, age {age_bin} years")
    
    description = ", ".join(description_parts) + "."
    
    return description, content_analysis


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--manifest", type=str, default="data/processed/manifest.csv")
    parser.add_argument("--num_samples", type=int, default=5)
    parser.add_argument("--output_dir", type=str, default="enhanced_genai_results")
    args = parser.parse_args()
    
    df = pd.read_csv(args.manifest)
    if "processed_path" in df.columns:
        df = df[df["processed_path"].astype(str).str.len() > 0].reset_index(drop=True)
    
    os.makedirs(args.output_dir, exist_ok=True)
    
    print("="*70)
    print("ENHANCED GENAI PROJECT: Actual Video Content Analysis")
    print("="*70)
    print("\nThis version analyzes VIDEO CONTENT, not just metadata!")
    print("="*70 + "\n")
    
    results = []
    
    for i in range(min(args.num_samples, len(df))):
        row = df.iloc[i]
        video_path = row["processed_path"] if "processed_path" in row else row["file_path"]
        
        if not os.path.exists(video_path):
            continue
        
        print(f"Analyzing video {i+1}/{args.num_samples}...")
        print(f"  Video: {os.path.basename(video_path)}")
        
        # Analyze video content
        description, analysis = enhanced_video_to_text(video_path, row)
        
        print(f"  Description: {description}")
        print(f"  Content Analysis:")
        print(f"    - Motion intensity: {analysis.get('motion_intensity', 0):.2f}")
        print(f"    - Brightness: {analysis.get('brightness', 0):.2f}")
        print(f"    - Edge density: {analysis.get('edge_density', 0):.3f}")
        print(f"    - Frame consistency: {analysis.get('frame_consistency', 0):.2f}")
        print()
        
        results.append({
            "video_id": i,
            "video_path": video_path,
            "description": description,
            "content_analysis": analysis,
            "metadata": {
                "view": row.get("view"),
                "sex": row.get("sex"),
                "age_bin": row.get("age_bin")
            }
        })
    
    # Save results
    import json
    with open(os.path.join(args.output_dir, "enhanced_analysis.json"), "w") as f:
        json.dump(results, f, indent=2)
    
    print("="*70)
    print("âœ“ Enhanced analysis complete!")
    print(f"  Results saved to: {args.output_dir}/")
    print("="*70)


if __name__ == "__main__":
    main()




